{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3d0960-4660-43a5-9953-d9855d46bee4",
   "metadata": {},
   "source": [
    "## 1-4. 윤리적으로 웹 스크래핑/크롤링 진행하기\n",
    "\n",
    "- 올바르게 웹 스크래핑/크롤링을 진행하는 데에 도움을 주는 `robots.txt`에 대해서 알아봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5b082-e080-4393-a3c8-a2c1e629203d",
   "metadata": {},
   "source": [
    "### robots.txt?\n",
    "\n",
    "**robots.txt는 웹 사이트 및 웹 페이지를 수집하는 로봇들의 무단 접근을 방지하기 위해 만들어진 로봇 배제 표준(robots exclusion standard)이자 국제 권고안입니다.**\n",
    "  \n",
    "일부 스팸 봇이나 악성 목적을 지닌 가짜 클라이언트 로봇은 웹 사이트에 진짜 클라이언트처럼 접근합니다. 그리고 무단으로 웹 사이트 정보를 긁어가거나, 웹 서버에 부하를 줍니다.  \n",
    "이런 로봇들의 무분별한 접근을 통제하기 위해 마련된 것이 robots.txt입니다.\n",
    "그래서 가끔 웹 서버에 요청을 보내도 요청을 거부 당하는 일이 있습니다. 우리를 무단 봇으로 짐작하고 웹 서버에서 접근을 막는 것이죠. 그럼 우리는 브라우저에게 스팸 봇이 아니라 사람이라는 것을 알려주면 되겠죠?  \n",
    "\n",
    "이때 브라우저에게 전달하는 것이 **사용자 에이전트(user agent)** 정보입니다. \n",
    "\n",
    "나의 User Agent 확인해보기 : https://www.whatismybrowser.com/detect/what-is-my-user-agent/\n",
    "\n",
    "사용자 에이전트는 요청을 보내는 것의 주체를 나타내는 프로그램입니다. 웹 맥락에서는 브라우저, 웹 페이지를 수집하는 봇, 다운로드 관리자, 웹에 접근하는 다른 앱 모두 사용자 에이전트지요.  \n",
    "웹 서버에 요청할 때 사용자 에이전트 HTTP 헤더(user agent HTTP header)에 나의 브라우저 정보를 전달하면 웹 서버가 나를 진짜 사용자로 인식할 수 있게 됩니다.  \n",
    "사용자 에이전트 헤더를 설정하는 방법은 아래 기본 코드 프레임에서 살펴보겠습니다.\n",
    "\n",
    "웹 스크래핑을 할 때 원칙은 다음과 같습니다.\n",
    "\n",
    "1. 요청하고자 하는 서버에 과도한 부하를 주지 않는다.\n",
    "2. 가져온 정보를 사용할 때 저작권과 데이터베이스권에 위배되지 않는지 주의한다.  \n",
    "\n",
    "이 원칙들을 잘 지킨다면 건전한 사용자 에이전트가 될 수 있을 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db3f86a-159b-47ff-969b-ef608df2d6c8",
   "metadata": {},
   "source": [
    "### robots.txt 가져오기\n",
    "\n",
    "robots.txt는 웹 페이지의 메인 주소에 '/robots.txt'를 입력하면 확인 할 수 있습니다. 예를 들어 naver의 경우에는 www.naver.com/robots.txt 를 입력하면 됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0841a0e1-bd44-44c3-a532-a7eb04d6b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests 모듈을 불러온 후, 다음 웹사이트에 대한 robots.txt 정책을 확인해봅시다.\n",
    "# https://www.naver.com\n",
    "import requests\n",
    "\n",
    "res = requests.get(\"https://naver.com/robots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1061035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-agent: *\n",
      "Disallow: /\n",
      "Allow : /$\n",
      "Allow : /.well-known/privacy-sandbox-attestations.json\n"
     ]
    }
   ],
   "source": [
    "print(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f348b1f6-1600-43e8-b861-c598b50a0eb3",
   "metadata": {},
   "source": [
    "어떤 정보가 반환되었네요! 이를 한편 살펴봅시다.\n",
    "\n",
    "- 'User-agent' : 규칙이 적용되는 대상 사용자 에이전트\n",
    "- 'Disallow' : 크롤링을 금지할 웹 페이지\n",
    "- 'Allow' : 크롤링을 허용할 웹 페이지\n",
    "\n",
    "자세한 규약은 robots.txt [공식 홈페이지](\"www.robotstxt.org\")를 참조해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b904d",
   "metadata": {},
   "source": [
    "### Exercise: 여러분이 자주 사용하시는 사이트의 robots.txt도 확인해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34dfe905-471e-4965-b624-8d87dd22ba89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# robots.txt file for YouTube\n",
      "# Created in the distant future (the year 2000) after\n",
      "# the robotic uprising of the mid 90's which wiped out all humans.\n",
      "\n",
      "User-agent: Mediapartners-Google*\n",
      "Disallow:\n",
      "\n",
      "User-agent: *\n",
      "Disallow: /api/\n",
      "Disallow: /comment\n",
      "Disallow: /feeds/videos.xml\n",
      "Disallow: /get_video\n",
      "Disallow: /get_video_info\n",
      "Disallow: /get_midroll_info\n",
      "Disallow: /live_chat\n",
      "Disallow: /login\n",
      "Disallow: /qr\n",
      "Disallow: /results\n",
      "Disallow: /signup\n",
      "Disallow: /t/terms\n",
      "Disallow: /timedtext_video\n",
      "Disallow: /verify_age\n",
      "Disallow: /watch_ajax\n",
      "Disallow: /watch_fragments_ajax\n",
      "Disallow: /watch_popup\n",
      "Disallow: /watch_queue_ajax\n",
      "Disallow: /youtubei/\n",
      "\n",
      "Sitemap: https://www.youtube.com/sitemaps/sitemap.xml\n",
      "Sitemap: https://www.youtube.com/product/sitemap.xml\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여러분이 자주 사용하는 사이트의 robots를 동일한 방법으로 확인해보세요.\n",
    "res = requests.get(\"https://youtube.com/robots.txt\")\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f005d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
